{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#from keras.datasets import mnist\n",
    "from keras import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "from skimage import io\n",
    "import os\n",
    "import scipy.misc\n",
    "from scipy.misc import imread, imresize\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set\n",
      "(32460,)\n",
      "(32460, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "datasets_path = 'C:/Users/Pushkar/Downloads/Datasets/omniglot/python'\n",
    "\n",
    "train_path = os.path.join(datasets_path,'images_background')\n",
    "evaluation_path = os.path.join(datasets_path,'images_evaluation')\n",
    "\n",
    "def load_images(path,n=0):\n",
    "    X = []\n",
    "    Y=[]\n",
    "    i=-1\n",
    "    \n",
    "    for back in os.listdir(path):\n",
    "        back_path = os.path.join(path,back)\n",
    "        for language in os.listdir(back_path):\n",
    "            #print (\"loading alphabet:\" + alphabet)\n",
    "            #Y.append(alphabet)\n",
    "            alphabet_path = os.path.join(back_path,language)\n",
    "            for letter in os.listdir(alphabet_path):\n",
    "                category_images = []\n",
    "                i=i+1\n",
    "                letter_path = os.path.join(alphabet_path,letter)\n",
    "\n",
    "                for filename in os.listdir(letter_path):\n",
    "                    image_path = os.path.join(letter_path,filename)\n",
    "                    image = imread(image_path)\n",
    "                    #Y.append(image)\n",
    "                    image = imresize(image,(28,28))\n",
    "                    image = image/255\n",
    "                    image = 1-image\n",
    "                    Y.append(i)\n",
    "                    X.append(image)\n",
    "    return X,Y\n",
    "\n",
    "print(\"training set\")\n",
    "x_train,y_train = load_images(datasets_path)\n",
    "\n",
    "\n",
    "X = np.array(x_train)\n",
    "y = np.array(y_train)\n",
    "#print(y)\n",
    "print(y.shape)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29214, 28, 28)\n",
      "(3246, 28, 28)\n",
      "[ 100 1222  681 ...,   43  789 1182]\n",
      "[1110  250 1268 ...,  116  601  445]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1549ed35ac8>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1549c893860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.1,random_state = 42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "plt.imshow(X_train[4],cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Class 354')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlbGpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15445d19400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[12], cmap='gray')\n",
    "plt.title('Class '+ str(y_train[12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, array([ 0.,  0.,  0., ...,  0.,  0.,  0.], dtype=float32))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_classes = 1623\n",
    "Y_train = np_utils.to_categorical(y_train, number_of_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, number_of_classes)\n",
    "\n",
    "\n",
    "(y_test[1],Y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(28,28,1)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64,(3,3 )))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(1623))\n",
    "\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_133 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_134 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_134 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_135 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_65 (MaxPooling (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_135 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_136 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_136 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_137 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_66 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_138 (Bat (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1623)              832599    \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 1623)              0         \n",
      "=================================================================\n",
      "Total params: 1,425,207\n",
      "Trainable params: 1,423,799\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gen = ImageDataGenerator()\n",
    "\n",
    "test_gen = ImageDataGenerator()\n",
    "train_generator = gen.flow(X_train, Y_train, batch_size=32)\n",
    "test_generator = test_gen.flow(X_test, Y_test, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "913/913 [==============================] - 175s 192ms/step - loss: 7.5246 - acc: 0.0111 - val_loss: 5.0603 - val_acc: 0.1063\n",
      "Epoch 2/100\n",
      "913/913 [==============================] - 174s 191ms/step - loss: 5.6072 - acc: 0.0609 - val_loss: 3.9897 - val_acc: 0.2461\n",
      "Epoch 3/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 4.6027 - acc: 0.1286 - val_loss: 2.9264 - val_acc: 0.4100\n",
      "Epoch 4/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 3.8212 - acc: 0.2066 - val_loss: 2.2942 - val_acc: 0.5068\n",
      "Epoch 5/100\n",
      "913/913 [==============================] - 176s 192ms/step - loss: 3.2296 - acc: 0.2806 - val_loss: 1.8474 - val_acc: 0.5650\n",
      "Epoch 6/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 2.7659 - acc: 0.3513 - val_loss: 1.4828 - val_acc: 0.6396\n",
      "Epoch 7/100\n",
      "913/913 [==============================] - 175s 191ms/step - loss: 2.4246 - acc: 0.4106 - val_loss: 1.3261 - val_acc: 0.6657\n",
      "Epoch 8/100\n",
      "913/913 [==============================] - 177s 193ms/step - loss: 2.1457 - acc: 0.4651 - val_loss: 1.1837 - val_acc: 0.6895\n",
      "Epoch 9/100\n",
      "913/913 [==============================] - 177s 193ms/step - loss: 1.9081 - acc: 0.5079 - val_loss: 1.0713 - val_acc: 0.7113\n",
      "Epoch 10/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 1.7483 - acc: 0.5400 - val_loss: 1.0502 - val_acc: 0.7261\n",
      "Epoch 11/100\n",
      "913/913 [==============================] - 177s 193ms/step - loss: 1.6021 - acc: 0.5664 - val_loss: 0.9196 - val_acc: 0.7477\n",
      "Epoch 12/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 1.4995 - acc: 0.5910 - val_loss: 0.8912 - val_acc: 0.7505\n",
      "Epoch 13/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 1.3962 - acc: 0.6148 - val_loss: 0.8303 - val_acc: 0.7659\n",
      "Epoch 14/100\n",
      "913/913 [==============================] - 175s 192ms/step - loss: 1.2964 - acc: 0.6383 - val_loss: 0.8020 - val_acc: 0.7723\n",
      "Epoch 15/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 1.2360 - acc: 0.6523 - val_loss: 0.7491 - val_acc: 0.7939\n",
      "Epoch 16/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 1.1787 - acc: 0.6633 - val_loss: 0.7374 - val_acc: 0.7896\n",
      "Epoch 17/100\n",
      "913/913 [==============================] - 177s 193ms/step - loss: 1.1228 - acc: 0.6778 - val_loss: 0.7341 - val_acc: 0.7880\n",
      "Epoch 18/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 1.0670 - acc: 0.6890 - val_loss: 0.7466 - val_acc: 0.7865\n",
      "Epoch 19/100\n",
      "913/913 [==============================] - 181s 198ms/step - loss: 1.0222 - acc: 0.7009 - val_loss: 0.6718 - val_acc: 0.8078\n",
      "Epoch 20/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.9780 - acc: 0.7104 - val_loss: 0.6495 - val_acc: 0.8130\n",
      "Epoch 21/100\n",
      "913/913 [==============================] - 178s 195ms/step - loss: 0.9510 - acc: 0.7197 - val_loss: 0.6665 - val_acc: 0.8047\n",
      "Epoch 22/100\n",
      "913/913 [==============================] - 178s 195ms/step - loss: 0.9166 - acc: 0.7247 - val_loss: 0.6392 - val_acc: 0.8198\n",
      "Epoch 23/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.8898 - acc: 0.7345 - val_loss: 0.6336 - val_acc: 0.8213\n",
      "Epoch 24/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.8559 - acc: 0.7423 - val_loss: 0.6274 - val_acc: 0.8213\n",
      "Epoch 25/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 0.8250 - acc: 0.7546 - val_loss: 0.6213 - val_acc: 0.8198\n",
      "Epoch 26/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.8046 - acc: 0.7538 - val_loss: 0.6354 - val_acc: 0.8130\n",
      "Epoch 27/100\n",
      "913/913 [==============================] - 179s 196ms/step - loss: 0.7828 - acc: 0.7631 - val_loss: 0.6104 - val_acc: 0.8229\n",
      "Epoch 28/100\n",
      "913/913 [==============================] - 178s 195ms/step - loss: 0.7653 - acc: 0.7659 - val_loss: 0.6266 - val_acc: 0.8262\n",
      "Epoch 29/100\n",
      "913/913 [==============================] - 178s 195ms/step - loss: 0.7494 - acc: 0.7697 - val_loss: 0.5948 - val_acc: 0.8296\n",
      "Epoch 30/100\n",
      "913/913 [==============================] - 179s 196ms/step - loss: 0.7191 - acc: 0.7781 - val_loss: 0.5955 - val_acc: 0.8299\n",
      "Epoch 31/100\n",
      "913/913 [==============================] - 178s 195ms/step - loss: 0.7123 - acc: 0.7806 - val_loss: 0.6000 - val_acc: 0.8312\n",
      "Epoch 32/100\n",
      "913/913 [==============================] - 178s 195ms/step - loss: 0.6935 - acc: 0.7865 - val_loss: 0.6166 - val_acc: 0.8309\n",
      "Epoch 33/100\n",
      "913/913 [==============================] - 178s 195ms/step - loss: 0.6825 - acc: 0.7874 - val_loss: 0.6059 - val_acc: 0.8269\n",
      "Epoch 34/100\n",
      "913/913 [==============================] - 178s 195ms/step - loss: 0.6665 - acc: 0.7897 - val_loss: 0.5969 - val_acc: 0.8324\n",
      "Epoch 35/100\n",
      "913/913 [==============================] - 179s 196ms/step - loss: 0.6493 - acc: 0.7973 - val_loss: 0.6125 - val_acc: 0.8284\n",
      "Epoch 36/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 0.6379 - acc: 0.8022 - val_loss: 0.5831 - val_acc: 0.8309\n",
      "Epoch 37/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.6227 - acc: 0.8048 - val_loss: 0.5849 - val_acc: 0.8355\n",
      "Epoch 38/100\n",
      "913/913 [==============================] - 178s 195ms/step - loss: 0.6136 - acc: 0.8093 - val_loss: 0.5727 - val_acc: 0.8420\n",
      "Epoch 39/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.6033 - acc: 0.8080 - val_loss: 0.5794 - val_acc: 0.8364\n",
      "Epoch 40/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 0.5971 - acc: 0.8095 - val_loss: 0.5876 - val_acc: 0.8312\n",
      "Epoch 41/100\n",
      "913/913 [==============================] - 176s 192ms/step - loss: 0.5808 - acc: 0.8175 - val_loss: 0.5738 - val_acc: 0.8364\n",
      "Epoch 42/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.5814 - acc: 0.8137 - val_loss: 0.5891 - val_acc: 0.8432\n",
      "Epoch 43/100\n",
      "913/913 [==============================] - 177s 193ms/step - loss: 0.5722 - acc: 0.8198 - val_loss: 0.5732 - val_acc: 0.8438\n",
      "Epoch 44/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.5540 - acc: 0.8231 - val_loss: 0.5764 - val_acc: 0.8423\n",
      "Epoch 45/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.5487 - acc: 0.8262 - val_loss: 0.5905 - val_acc: 0.8395\n",
      "Epoch 46/100\n",
      "913/913 [==============================] - 176s 192ms/step - loss: 0.5332 - acc: 0.8279 - val_loss: 0.5911 - val_acc: 0.8407\n",
      "Epoch 47/100\n",
      "913/913 [==============================] - 177s 193ms/step - loss: 0.5307 - acc: 0.8298 - val_loss: 0.5944 - val_acc: 0.8417\n",
      "Epoch 48/100\n",
      "913/913 [==============================] - 178s 195ms/step - loss: 0.5257 - acc: 0.8301 - val_loss: 0.5689 - val_acc: 0.8478\n",
      "Epoch 49/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.5109 - acc: 0.8346 - val_loss: 0.5757 - val_acc: 0.8484\n",
      "Epoch 50/100\n",
      "913/913 [==============================] - 178s 195ms/step - loss: 0.5022 - acc: 0.8387 - val_loss: 0.5863 - val_acc: 0.8407\n",
      "Epoch 51/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 0.4991 - acc: 0.8380 - val_loss: 0.5859 - val_acc: 0.8395\n",
      "Epoch 52/100\n",
      "913/913 [==============================] - 179s 196ms/step - loss: 0.4903 - acc: 0.8393 - val_loss: 0.5791 - val_acc: 0.8401\n",
      "Epoch 53/100\n",
      "913/913 [==============================] - 178s 195ms/step - loss: 0.4677 - acc: 0.8484 - val_loss: 0.5926 - val_acc: 0.8407\n",
      "Epoch 54/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.4832 - acc: 0.8439 - val_loss: 0.5781 - val_acc: 0.8380\n",
      "Epoch 55/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.4744 - acc: 0.8490 - val_loss: 0.5897 - val_acc: 0.8453\n",
      "Epoch 56/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.4515 - acc: 0.8521 - val_loss: 0.6133 - val_acc: 0.8410\n",
      "Epoch 57/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.4502 - acc: 0.8547 - val_loss: 0.5820 - val_acc: 0.8450\n",
      "Epoch 58/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.4590 - acc: 0.8503 - val_loss: 0.5870 - val_acc: 0.8450\n",
      "Epoch 59/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.4586 - acc: 0.8500 - val_loss: 0.5893 - val_acc: 0.8407\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "913/913 [==============================] - 176s 192ms/step - loss: 0.4502 - acc: 0.8527 - val_loss: 0.5870 - val_acc: 0.8401\n",
      "Epoch 61/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 0.4409 - acc: 0.8545 - val_loss: 0.5665 - val_acc: 0.8478\n",
      "Epoch 62/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.4341 - acc: 0.8574 - val_loss: 0.5596 - val_acc: 0.8481\n",
      "Epoch 63/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 0.4356 - acc: 0.8583 - val_loss: 0.6307 - val_acc: 0.8413\n",
      "Epoch 64/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.4213 - acc: 0.8610 - val_loss: 0.5988 - val_acc: 0.8426\n",
      "Epoch 65/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.4273 - acc: 0.8593 - val_loss: 0.5837 - val_acc: 0.8435\n",
      "Epoch 66/100\n",
      "913/913 [==============================] - 177s 193ms/step - loss: 0.4096 - acc: 0.8649 - val_loss: 0.5942 - val_acc: 0.8447\n",
      "Epoch 67/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.4131 - acc: 0.8643 - val_loss: 0.5736 - val_acc: 0.8475\n",
      "Epoch 68/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 0.4092 - acc: 0.8661 - val_loss: 0.5683 - val_acc: 0.8487\n",
      "Epoch 69/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.3924 - acc: 0.8704 - val_loss: 0.5706 - val_acc: 0.8487\n",
      "Epoch 70/100\n",
      "913/913 [==============================] - 176s 192ms/step - loss: 0.3971 - acc: 0.8700 - val_loss: 0.5903 - val_acc: 0.8457\n",
      "Epoch 71/100\n",
      "913/913 [==============================] - 178s 195ms/step - loss: 0.3912 - acc: 0.8705 - val_loss: 0.6083 - val_acc: 0.8370\n",
      "Epoch 72/100\n",
      "913/913 [==============================] - 177s 193ms/step - loss: 0.3925 - acc: 0.8719 - val_loss: 0.5847 - val_acc: 0.8490\n",
      "Epoch 73/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 0.3894 - acc: 0.8721 - val_loss: 0.5860 - val_acc: 0.8503\n",
      "Epoch 74/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.3750 - acc: 0.8762 - val_loss: 0.5845 - val_acc: 0.8552\n",
      "Epoch 75/100\n",
      "913/913 [==============================] - 177s 193ms/step - loss: 0.3884 - acc: 0.8714 - val_loss: 0.5852 - val_acc: 0.8450\n",
      "Epoch 76/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.3734 - acc: 0.8750 - val_loss: 0.5821 - val_acc: 0.8484\n",
      "Epoch 77/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 0.3599 - acc: 0.8804 - val_loss: 0.5697 - val_acc: 0.8521\n",
      "Epoch 78/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 0.3783 - acc: 0.8746 - val_loss: 0.5723 - val_acc: 0.8571\n",
      "Epoch 79/100\n",
      "913/913 [==============================] - 175s 192ms/step - loss: 0.3649 - acc: 0.8790 - val_loss: 0.6133 - val_acc: 0.8457\n",
      "Epoch 80/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.3625 - acc: 0.8823 - val_loss: 0.5775 - val_acc: 0.8527\n",
      "Epoch 81/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.3592 - acc: 0.8816 - val_loss: 0.5909 - val_acc: 0.8509\n",
      "Epoch 82/100\n",
      "913/913 [==============================] - 177s 193ms/step - loss: 0.3533 - acc: 0.8829 - val_loss: 0.5866 - val_acc: 0.8497\n",
      "Epoch 83/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.3546 - acc: 0.8807 - val_loss: 0.6033 - val_acc: 0.8472\n",
      "Epoch 84/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 0.3513 - acc: 0.8816 - val_loss: 0.5862 - val_acc: 0.8500\n",
      "Epoch 85/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 0.3498 - acc: 0.8822 - val_loss: 0.5945 - val_acc: 0.8503\n",
      "Epoch 86/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 0.3411 - acc: 0.8863 - val_loss: 0.5874 - val_acc: 0.8494\n",
      "Epoch 87/100\n",
      "913/913 [==============================] - 177s 193ms/step - loss: 0.3352 - acc: 0.8858 - val_loss: 0.5817 - val_acc: 0.8540\n",
      "Epoch 88/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.3410 - acc: 0.8857 - val_loss: 0.5880 - val_acc: 0.8475\n",
      "Epoch 89/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.3388 - acc: 0.8866 - val_loss: 0.5724 - val_acc: 0.8534\n",
      "Epoch 90/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 0.3326 - acc: 0.8883 - val_loss: 0.5838 - val_acc: 0.8530\n",
      "Epoch 91/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.3245 - acc: 0.8920 - val_loss: 0.5888 - val_acc: 0.8490\n",
      "Epoch 92/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 0.3278 - acc: 0.8907 - val_loss: 0.6026 - val_acc: 0.8534\n",
      "Epoch 93/100\n",
      "913/913 [==============================] - 176s 192ms/step - loss: 0.3344 - acc: 0.8892 - val_loss: 0.5842 - val_acc: 0.8512\n",
      "Epoch 94/100\n",
      "913/913 [==============================] - 175s 192ms/step - loss: 0.3227 - acc: 0.8913 - val_loss: 0.5727 - val_acc: 0.8552\n",
      "Epoch 95/100\n",
      "913/913 [==============================] - 177s 194ms/step - loss: 0.3209 - acc: 0.8911 - val_loss: 0.5939 - val_acc: 0.8534\n",
      "Epoch 96/100\n",
      "913/913 [==============================] - 176s 192ms/step - loss: 0.3153 - acc: 0.8927 - val_loss: 0.5766 - val_acc: 0.8592\n",
      "Epoch 97/100\n",
      "913/913 [==============================] - 177s 193ms/step - loss: 0.3150 - acc: 0.8949 - val_loss: 0.5658 - val_acc: 0.8555\n",
      "Epoch 98/100\n",
      "913/913 [==============================] - 175s 192ms/step - loss: 0.3198 - acc: 0.8953 - val_loss: 0.5766 - val_acc: 0.8552\n",
      "Epoch 99/100\n",
      "913/913 [==============================] - 175s 192ms/step - loss: 0.3061 - acc: 0.8983 - val_loss: 0.5920 - val_acc: 0.8518\n",
      "Epoch 100/100\n",
      "913/913 [==============================] - 176s 193ms/step - loss: 0.3049 - acc: 0.8983 - val_loss: 0.5950 - val_acc: 0.8555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1549ed614a8>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, epochs=100, \n",
    "                    validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3246/3246 [==============================] - 6s 2ms/step\n",
      "\n",
      "Test loss:  0.594996739484\n",
      "Test Accuracy 0.855514479433\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print()\n",
    "print('Test loss: ', score[0])\n",
    "print('Test Accuracy', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predictions = model.predict_classes(X_test)\n",
    "\n",
    "predictions = list(predictions)\n",
    "actuals = list(y_test)\n",
    "\n",
    "sub = pd.DataFrame({'Actual': actuals, 'Predictions': predictions})\n",
    "sub.to_csv('./output_cnn.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
